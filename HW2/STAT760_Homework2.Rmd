---
title: "STAT760_Homework2"
author: "Natalie Bladis, Johnny Lomas"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r include=FALSE}
#setwd("/Users/ntb3/Documents/STAT_760_Statistical_Learning/Statistical_Learning")
#setwd("/Users/jslomas/Box/STAT_760/Statistical_Learning")
```
## Exercise 1. (ESL Ex. 3.12) – 3 pts
Show that the ridge regression estimates can be obtained by ordinary least squares regression
on an augmented data set. We augment the centered matrix X with p additional rows $\sqrt{\lambda{I}}$, and
augment y with p zeros. By introducing artificial data having response value zero, the fitting
procedure is forced to shrink the coefficients toward zero. This is related to the idea of hints due to
Abu-Mostafa (1995), where model constraints are implemented by adding artificial data examples
that satisfy them.

## Solution:
Let $X^*=\begin{bmatrix}X \\\sqrt{\lambda}I_p\end{bmatrix}$ and $y^* = \begin{bmatrix}y \\0_p\end{bmatrix}$ where $I_p$ is a $p \times p$ identify matrix and $0_p$ is a $p \times 1$ vector of zeros. 

Then 

$$
\begin{aligned}
X^{*T}X^* &= \begin{bmatrix}X \sqrt{\lambda}I_p\end{bmatrix} \begin{bmatrix}X \\\sqrt{\lambda}I_p\end{bmatrix} \\
&= X^TX+\lambda I_p
\end{aligned}
$$
and 
$$
\begin{aligned}
X^{*T}y^* &= \begin{bmatrix}X \sqrt{\lambda}I_p\end{bmatrix} \begin{bmatrix} y \\0_p\end{bmatrix} \\
&= X^Ty+ \sqrt\lambda I_p 0_p \\
&= X^Ty
\end{aligned}
$$
Therefore, 
$$
\begin{aligned}
\hat{\beta}_{new} &= (X^{*T}X^*)^{-1}X^{*T}y^* \\
&= (X^Ty+ \lambda I_p)^{-1} X^Ty
\end{aligned}
$$
Which is the $\hat{\beta}$ estiamte for ridge regression. 

## Exercise 2. (Programming) – 12 pts
Use prostate cancer example to reproduce the following figure. The Prostate data are available
from the book website https://www.hastie.su.domains/ElemStatLearn/.


```{r}
setwd("/Users/ntb3/Documents/STAT_760_Statistical_Learning/Statistical_Learning/HW2")
#setwd("/Users/jslomas/Box/STAT_760/Statistical_Learning/HW2")
library(dplyr)
library(ggplot2)
library(data.table)

# Input (x): Augmented data matrix
# Input (y): Response vector
# Return: Fitted parameter vector (beta)
fitLeastSquares <- function(x, y){
   beta = solve(t(x) %*% x) %*% t(x) %*% y
   return(beta)
}

predictLeastSquares <- function(x, beta) {
  y_hat <- x %*% beta
  return(y_hat)
}

# Input (y)    : Original response vector
# Input (y_hat): Predicted response vector
# Return       : RSS value, scalar
computeRSS <- function(y, y_hat){
  rss <- t(y-y_hat) %*% (y-y_hat)
  return(rss)
}

prostate <- read.delim("prostate.tsv")
response <- as.matrix(prostate %>% select(lpsa))
data <- prostate %>% select(lcavol:pgg45)

plot_data <- data.frame()
# TODO: fit null case and add to plotting dataframe
null_rss <- sum((response - mean(response))^2)
plot_data <- rbind(plot_data, c(null_rss, 0))

for (feat in 1:ncol(data)){
  sets <- combn(1:ncol(data), feat)
  for (set in 1:ncol(sets)){
    data_subset <- data %>% select(sets[,set])
    data_subset <- cbind(rep(1, nrow(data)), data_subset)
    data_subset <- as.matrix(data_subset)
    
    beta_hat <- fitLeastSquares(data_subset, response)
    y_hat <- predictLeastSquares(data_subset, beta_hat)
    rss <- computeRSS(response, y_hat)
    
    # min_rss_vector <- c()
    # min_rss <- min(rss)
    # 
    # if (rss == min_rss) {
    # min_rss_vector == 1
    #   } else {
    #   min_rss_vector == 0
    #   }

    plot_data <- rbind(plot_data, c(rss, feat))
  }
  
}

colnames(plot_data) <- c("RSS", "K")

plot_data <- as.data.table(plot_data)
mins <- plot_data[ , .SD[which.min(RSS)], by = K]
plot_data <- cbind(plot_data, mins = plot_data$RSS %in% mins$RSS) %>%
  mutate(minID = ifelse(mins == TRUE, "Min", "Other")) %>%
  mutate(minID = relevel(factor(minID), ref = "Other")) %>%
  mutate(minID = factor(minID, levels = rev(levels(minID))))


ggplot(plot_data, aes(x=K, y=RSS, group = minID)) +
  geom_point(data = plot_data[plot_data$mins == F], aes(color = "Other")) +
  geom_point(data = plot_data[plot_data$mins == T], aes(color = "Min")) +
  geom_line(data = plot_data[plot_data$mins == T], aes(color = "Min")) +
  ylim(c(0,130)) +
  ylab("Residual Sum-of-Squares") +
  xlab("Subset size k") +
  scale_color_manual(values = c("red", "gray"))

```


## Exercise 3. (Programming) – 15 pts
Use prostate cancer example to produce a similar figure below (also Figure 3.7 in ESL), where
the x-axis is the parameter λinstead of degrees of freedom. The estimates of prediction errors
are obtained by 10-fold cross-validation. The Prostate data are available from the book website
https://www.hastie.su.domains/ElemStatLearn/.

1)Write a function to do ridge reg

```{r}
library(dplyr)


count = 1
sets = c()
for (i in 1:9){
  sets <- append(sets, rep(count, 10))
  count = count + 1
}

sets <- append(sets, rep(10, 7))
prostate <- read.delim("prostate.tsv") %>% 
  select(lcavol:lpsa) %>%
  cbind(sort = runif(nrow(prostate), 0, 1)) %>%
  arrange(sort) %>%
  select(lcavol:lpsa) %>%
  scale(TRUE, TRUE) %>%
  cbind(sets = sets)
  

prostate <- as.data.frame(prostate)


# Input (x): Augmented data matrix
# Input (y): Response vector
# Input (l): lambda value
# Input (I)
# Return: Fitted parameter vector (beta)

fitRidgeRegression <- function(x, y, l){
   I = as.matrix(diag(ncol(x)))
   beta_ridge = solve(t(x) %*% x + l*I) %*% t(x) %*% y
   return(beta_ridge)
}

predictRidgeReg <- function(x, beta_ridge) {
  y_hat <- x %*% beta_ridge
  return(y_hat)
}


CV_error <- data.frame()

lambda <- c(2,10,25,50,100,250, 500, 750,1000)
lambda <- log(lambda)

for (l in lambda){
  errors <- c()
  for (test in 1:10){
    train_set <- prostate %>% filter(sets != test)
    test_set <- prostate %>% filter(sets == test)
    x_train <- as.matrix(train_set %>% select(lcavol:pgg45))
    y_train <- as.matrix(train_set %>% select(lpsa))
  
    x_test <- as.matrix(test_set %>% select(lcavol:pgg45))
    y_test <- as.matrix(test_set %>% select(lpsa))
  
    beta_ridge_response <- fitRidgeRegression(x_train, y_train, l)
    ridge_reg_hat <- predictRidgeReg(x_test, beta_ridge_response)
    
    
    
    mse <- mean((ridge_reg_hat - y_test)^2)
    errors <- append(errors, mse)
    
  }
  mse_mean <- mean(errors)
  mse_min <- min(errors)
  mse_max <- max(errors)
  mse_sd_plus <- mean(errors) + sd(errors)
  mse_sd_minus <- mean(errors) - sd(errors)
  
  
  CV_error <- rbind(CV_error, c(l, mse_mean, mse_min, mse_max, mse_sd_minus, mse_sd_plus )) # to change x-axis to df make first input 1/l (d_i^2)/(d_i^2-lambda)
}

colnames(CV_error) <- c("lambda", "CV", "Min_MSE", "Max_MSE", "minus_sd", "plus_sd")




ggplot(CV_error, aes(x=lambda, y=CV)) +
  geom_point() +
  #geom_errorbar(aes(x=lambda, ymin=Min_MSE, ymax=Max_MSE), width=0.4, colour="grey", alpha=0.9, size=1.3) +
  geom_errorbar(aes(x=lambda, ymin=minus_sd, ymax=plus_sd), width=0.4, colour="grey", alpha=0.9, size=1.3) +
  #ylim(c(0,1)) +
  ylab("CV Error") +
  xlab("lambda")
  



```

Questions 

1) Does question 1 look ok?
2) Do we need red dots for minimum and the red lines connecting the minimums for Q2
3) Q3 = ok? ways to choose lambda? do we need to scale the data? re - book example, do we need to create the confidence interval like bars? Is the best way to choose the lambda value with the df formula and singular value decomposition?

box plot of 10 fold cross valifidation mse, add red minimal dots to Q2 the x-axis should be df = 1/lambda