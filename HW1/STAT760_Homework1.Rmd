---
title: "STAT760_Homework1"
author: "Natalie Bladis"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---


# Exercise 1. (ESL Ex. 2.1) – 5 pts
Suppose each of K classes has an associated target tk, which is a vector of all zeros,
except a one in the kth position. Show that classifying to the largest element of ˆy amounts to choosing the closest target, mink ‖tk −ˆy‖, if the elements of ˆy sum to one.

## Solution:
Need to show $\underset{k}{argmin}||t_k-\hat{y}||=\underset{k}{argmax}(y_k)$:
$$\begin{aligned}
\underset{k}{argmin}||t_k-\hat{y}|| &= \underset{k}{argmin}||t_k-\hat{y}||^2 \ \ \ \ \ \ \ \  \text{by symmetry of the norm and because }x \to x^2 \ is  \ monotonic , \\
&= \underset{k}{argmin} \sqrt{\sum_{i=1}^{k}{(y_i-(t_k)_i)^2}} \ \ \ \ \ \ \ \text{by the definition of norm} ,\\
&= \underset{k}{argmin} \sum_{i=1}^{k}{(y_i-(t_k)_i)^2} \ \ \ \ \ \ \ \text{argmin will be equivalent}, \sqrt{} \text{can be ignored}, \\
&= \underset{k}{argmin} \sum_{i=1}^{k}{(y_i^2-2y_i(t_k)_i)+(t_k)_i^2)} \ \ \ \ \ \ \ \text{distibution} \\
&= \underset{k}{argmin} [\sum_{i=1}^{k}{y_i^2}+\sum_{i=1}^{k}{(-2y_i(t_k)_i)+(t_k)_i^2)}] \ \ \ \ \ \ \ \text{properties of summation} \\
&= \underset{k}{argmin}\sum_{i=1}^{k}{(-2y_i(t_k)_i)+(t_k)_i^2)} \ \ \ \ \ \ \ \text{since the sum} \sum_{i=1}^{k}{y_i^2} \text{is the same for all classes k}, \\
&= \underset{k}{argmin}{(-2y_k+1)} \ \ \ \ \ \ \ \text{since for each k}, \sum_{i=1}^{k}{(t_k)_i^2}=1, \text{and} \sum_{i=1}^{k}{(-2y_i(t_k)_i)}=y_k \\
&= \underset{k}{argmin}{(-2y_k)} \ \ \ \ \ \ \ \\
&= \underset{k}{argmax}{(y_k)}.
\end{aligned}$$

# Exercise 2. (ESL Ex. 2.3) – 5 pts
Consider N data points uniformly distributed in a p-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median distance from the origin to the closest data point is given by the expression
$$d(p, N ) = {(1 − \frac{1}{2}^{\frac{1}{N}})}^{\frac{1}{p}}$$.
Derive equation (1).,

## Solution:

Let $X$ be a random variable representing the $N$ data points such that 

$$ \begin{aligned}
X = \underset{i \in {1,2,...,N}}{min}||x_i||
\end{aligned}
$$
so that the $x_i$ are uniformly distributed in a p-dimensional unit ball centered at the origin and $||.||$ represents the Euclidean norm. 

Let $m$ be the median distance from the origin to the closest data point. Therefore, 
#P(\text{All N points are further than m from the origin})=\frac{1}{2}
$$ \begin{aligned}
P(X \leq m) = \frac{1}{2} \\
1-P(X > m) = \frac{1}{2}
\end{aligned} \\

$$
by the definition of the median. The points $x_i$ are independent distributed over the ball, so

$$ \begin{aligned}
1-P(X > m) &= \frac{1}{2} \\
1-\prod_{i=1}^{N}P(||x_i|| > m) &= \frac{1}{2} \\
\frac{1}{2}&= \prod_{i=1}^{N}P(||x_i|| > m)
\end{aligned}
$$
Because the points $x_i$ are uniformly distributed in the unit ball in  p-dimensions, 

$$ \begin{aligned}
P(||x_i|| > m) &= 1 - P(||x_i|| \leq m) \\
\end{aligned}
$$
Because the points are uniformly distributed over the unit ball in p-dimensions, 

$$\begin{aligned}
P(||x_i|| \leq m)&=\frac{\text{volume of the p-ball with radius m centered at origin}}{\text{volume of whole ball with radius 1 centered at the origin}} \\
&= \frac{\pi m^p}{\pi 1^p} \\
&= m^p
\end{aligned}
$$
and therefore

$$
\begin{aligned}
P(||x_i|| \geq m) &= 1 - P(||x_i|| < m) \\
&= 1-m^p
\end{aligned}
$$

So,

$$ 
\begin{aligned}
\frac{1}{2} &= P(X \geq m)  \\
\frac{1}{2} &= \prod_{i=1}^{N}P(||x_i|| > m) \\
\frac{1}{2} &= (1-m^p)^N
\end{aligned}
$$
Solving for $m$ yields, 

$$ 
\begin{aligned}
m = {(1 − \frac{1}{2}^{\frac{1}{N}})}^{\frac{1}{p}}
\end{aligned}
$$

# Exercise 3. (ESL Ex. 2.8) – 20 pts
Note: Please write your own code. Don’t use libraries or packages.
<<<<<<< HEAD
Compare the classification performance of linear regression and k-nearest neighbor classification on the zipcode data. In particular, consider only the 2’s and 3’s, and k = 1, 3, 5, 7 and 15. Show both the training and test error for each choice. The zipcode data are available from the book website https://www.hastie.su.domains/ElemStatLearn/.

=======
Compare the classification performance of linear regression and k-nearest neighbor clas-
sification on the zipcode data. In particular, consider only the 2’s and 3’s, and k = 1, 3, 5, 7
and 15. Show both the training and test error for each choice. The zipcode data are
available from the book website https://www.hastie.su.domains/ElemStatLearn/.

## Solution

```{r setup, include=FALSE}
knitr::knit_engines$set(python = reticulate::eng_python)
```

```{python}
import numpy as np

def trainNormalEquation(x: np.matrix, y: np.array) -> np.array:
	beta = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)
	return (beta)

def classifyLinReg(beta, x):
	y = x.dot(beta)
	y = (y > 0.5).astype(int)
	return(y)

#def distance(d1: np.array, d2: np.array) -> int:
	# TODO: calculate distance in 256 

#def classifyKNN(k: int, new_data: np.matrix, data: np.matrix) -> int:
	#TODO

def importData(file: str, addOnes: bool):
	# Import data to a matrix
	matrix = np.loadtxt(file)
	
	# get only 2s and 3s
	mask = np.logical_or(matrix[:, 0] == 2, matrix[:, 0] == 3)
	matrix = matrix[mask]
	
	# code 2 -> 0, 3 -> 1
	y_vec = matrix[:, 0] == 3
	y_vec = y_vec.astype(int)

	if addOnes:
		matrix[:, 0] = np.ones(len(matrix))
		x_mat = matrix[:, 0:256]
	else:
		x_mat = matrix[:, 1:256]
	
	return(x_mat, y_vec)

def computeError(truth, prediction):
	return(1 - sum(truth == prediction)/len(truth))

x, y = importData('zip.train', True)
beta = trainNormalEquation(x, y)
classified = classifyLinReg(beta, x)
error = computeError(y, classified)
print(f"\nLinear regression error (training set): {error}")

x, y = importData('zip.test', True)
classified = classifyLinReg(beta, x)
error = computeError(y, classified)
print(f"Linear regression error (test sest): {error}")
```
>>>>>>> d5b4d77bb08ac6c2d3c8c659cf30d412261aa236
